{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hc4293/miniconda3/envs/nlpenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SUBMISSION: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.21it/s]\n",
      "Embedding: 100%|██████████| 1294/1294 [00:57<00:00, 22.42it/s]\n",
      "Embedding: 100%|██████████| 1752/1752 [03:05<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAP@25: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 434/434 [00:45<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAP@25: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 5/5 [00:00<00:00, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# Dependencies\n",
    "##############################################\n",
    "import os, math, numpy as np\n",
    "import pandas as pd\n",
    "import re, gc\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "\n",
    "##############################################\n",
    "# Configuration\n",
    "##############################################\n",
    "IS_SUBMISSION = False  # Set to False to do train/val evaluation. Later, switch to True for final submission\n",
    "base_model_path = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "query_max_len, doc_max_len = 320, 48\n",
    "task = \"Given a math multiple-choice problem with a student's wrong answer, retrieve the math misconceptions\"\n",
    "K = 25  # Top K misconceptions\n",
    "\n",
    "print('IS_SUBMISSION:', IS_SUBMISSION)\n",
    "\n",
    "##############################################\n",
    "# Loading Data\n",
    "##############################################\n",
    "df_full_train = pd.read_csv(\"./data/train.csv\").fillna(-1)\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "df_misconception_mapping = pd.read_csv(\"./data/misconception_mapping.csv\")\n",
    "\n",
    "# Create train/validation split from full training data\n",
    "df_train_split, df_val = train_test_split(df_full_train, test_size=0.2, random_state=42)\n",
    "df_train_split = df_train_split.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "# Depending on scenario:\n",
    "# If we want submission: we'll set IS_SUBMISSION = True and use df_test.\n",
    "# For now, we keep IS_SUBMISSION = False to evaluate on train/val.\n",
    "\n",
    "##############################################\n",
    "# Prompt Formatting Function\n",
    "##############################################\n",
    "TEMPLATE_INPUT_V3 = '{QUESTION}\\nCorrect answer: {CORRECT_ANSWER}\\nStudent wrong answer: {STUDENT_WRONG_ANSWER}'\n",
    "\n",
    "def format_input_v3(row, wrong_choice):\n",
    "    assert wrong_choice in \"ABCD\"\n",
    "    question_text = row.get(\"QuestionText\", \"No question text provided\")\n",
    "    subject_name = row.get(\"SubjectName\", \"Unknown subject\")\n",
    "    construct_name = row.get(\"ConstructName\", \"Unknown construct\")\n",
    "    correct_answer = row.get(\"CorrectAnswer\", \"Unknown\")\n",
    "    assert wrong_choice != correct_answer\n",
    "    correct_answer_text = row.get(f\"Answer{correct_answer}Text\", \"No correct answer text available\")\n",
    "    wrong_answer_text = row.get(f\"Answer{wrong_choice}Text\", \"No wrong answer text available\")\n",
    "\n",
    "    formatted_question = f\"\"\"Question: {question_text}\n",
    "    \n",
    "SubjectName: {subject_name}\n",
    "ConstructName: {construct_name}\"\"\"\n",
    "\n",
    "    ret = {\n",
    "        \"QUESTION\": formatted_question,\n",
    "        \"CORRECT_ANSWER\": correct_answer_text,\n",
    "        \"STUDENT_WRONG_ANSWER\": wrong_answer_text,\n",
    "        \"MISCONCEPTION_ID\": row.get(f'Misconception{wrong_choice}Id'),\n",
    "    }\n",
    "    ret[\"PROMPT\"] = TEMPLATE_INPUT_V3.format(**ret)\n",
    "    return ret\n",
    "\n",
    "##############################################\n",
    "# Data Preparation Function\n",
    "##############################################\n",
    "def prepare_input_df(df, is_submission):\n",
    "    items = []\n",
    "    target_ids = []\n",
    "    for _, row in df.iterrows():\n",
    "        for choice in ['A', 'B', 'C', 'D']:\n",
    "            if choice == row[\"CorrectAnswer\"]:\n",
    "                continue\n",
    "            if not is_submission and row.get(f'Misconception{choice}Id', -1) == -1:\n",
    "                # Skip if we don't have a known misconception ID in training/val (for evaluation)\n",
    "                continue\n",
    "            item = {'QuestionId_Answer': '{}_{}'.format(row['QuestionId'], choice)}\n",
    "            item['Prompt'] = format_input_v3(row, choice)['PROMPT']\n",
    "            items.append(item)\n",
    "            # Store ground truth ID if available\n",
    "            target_ids.append(int(row.get(f'Misconception{choice}Id', -1)))\n",
    "    df_input = pd.DataFrame(items)\n",
    "    return df_input, target_ids\n",
    "\n",
    "# Prepare train split input\n",
    "df_train_input, train_target_ids = prepare_input_df(df_train_split, is_submission=False)\n",
    "\n",
    "# Prepare validation input\n",
    "df_val_input, val_target_ids = prepare_input_df(df_val, is_submission=False)\n",
    "\n",
    "# Prepare test input\n",
    "# For test we are submitting predictions, so IS_SUBMISSION = True\n",
    "df_test_input, _ = prepare_input_df(df_test, is_submission=True)\n",
    "\n",
    "##############################################\n",
    "# Construct Queries and Documents\n",
    "##############################################\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}'\n",
    "\n",
    "def get_new_queries(queries, query_max_len, examples_prefix, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        queries,\n",
    "        max_length=query_max_len - len(tokenizer('<s>', add_special_tokens=False)['input_ids']) -\n",
    "        len(tokenizer('\\n<response></s>', add_special_tokens=False)['input_ids']),\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    prefix_ids = tokenizer(examples_prefix, add_special_tokens=False)['input_ids']\n",
    "    suffix_ids = tokenizer('\\n<response>', add_special_tokens=False)['input_ids']\n",
    "    new_max_length = (len(prefix_ids) + len(suffix_ids) + query_max_len + 8) // 8 * 8 + 8\n",
    "    new_queries = tokenizer.batch_decode(inputs['input_ids'])\n",
    "    for i in range(len(new_queries)):\n",
    "        new_queries[i] = examples_prefix + new_queries[i] + '\\n<response>'\n",
    "    return new_max_length, new_queries\n",
    "\n",
    "documents = df_misconception_mapping['MisconceptionName'].tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "examples_prefix = ''\n",
    "\n",
    "##############################################\n",
    "# Embedding Helper Functions\n",
    "##############################################\n",
    "MAX_LENGTH = query_max_len\n",
    "\n",
    "def last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "    batch_size = last_hidden_states.shape[0]\n",
    "    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_embeddings_in_batches(model, tokenizer, texts, max_length, batch_size=4):\n",
    "    embeddings = []\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_dict = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        batch_dict = {k: v.to(device) for k, v in batch_dict.items()}  # ensure all on same device\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_dict, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]  \n",
    "            batch_embeddings = last_token_pool(hidden_states, batch_dict[\"attention_mask\"])\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1).cpu()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "##############################################\n",
    "# Load the Qwen Model\n",
    "##############################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=None\n",
    ")\n",
    "\n",
    "##############################################\n",
    "# Compute Embeddings for Documents (Misconceptions)\n",
    "##############################################\n",
    "# We'll embed documents once since they'll be reused\n",
    "data_docs = documents\n",
    "doc_embeds = get_embeddings_in_batches(model, tokenizer, data_docs, max_length=MAX_LENGTH, batch_size=2)\n",
    "\n",
    "##############################################\n",
    "# Function to get predictions for a given df_input\n",
    "##############################################\n",
    "def get_predictions(df_input, doc_embeds, model, tokenizer):\n",
    "    queries = [get_detailed_instruct(task, q) for q in df_input['Prompt']]\n",
    "    # Generate tokenized queries\n",
    "    _, new_queries = get_new_queries(queries, query_max_len, examples_prefix, tokenizer)\n",
    "    # Embed queries\n",
    "    query_embeds = get_embeddings_in_batches(model, tokenizer, new_queries, max_length=MAX_LENGTH, batch_size=2)\n",
    "    # Compute similarity and get top K\n",
    "    scores = query_embeds @ doc_embeds.T\n",
    "    sorted_indices = torch.argsort(scores, dim=1, descending=True)[:, :K].tolist()\n",
    "    return sorted_indices\n",
    "\n",
    "##############################################\n",
    "# Evaluate Predictions using MAP@K\n",
    "##############################################\n",
    "def map_at_k(y_true, y_pred, k=25):\n",
    "    \"\"\"Compute Mean Average Precision at K for each sample.\"\"\"\n",
    "    average_precisions = []\n",
    "    for true, preds in zip(y_true, y_pred):\n",
    "        # preds is a list of predicted misconception IDs\n",
    "        # we have only one relevant misconception per QA pair in this scenario\n",
    "        # Check if 'true' is in top K\n",
    "        if true in preds[:k]:\n",
    "            rank = preds.index(true) + 1  # 1-based rank\n",
    "            # AP = 1/rank since there's only one relevant item\n",
    "            ap = 1.0 / rank\n",
    "        else:\n",
    "            ap = 0.0\n",
    "        average_precisions.append(ap)\n",
    "    return np.mean(average_precisions) if average_precisions else 0.0\n",
    "\n",
    "##############################################\n",
    "# Get predictions and evaluate for Train Split\n",
    "##############################################\n",
    "train_sorted_indices = get_predictions(df_train_input, doc_embeds, model, tokenizer)\n",
    "train_map25 = map_at_k(train_target_ids, train_sorted_indices, k=25)\n",
    "print(f\"Train MAP@25: {train_map25:.4f}\")\n",
    "\n",
    "##############################################\n",
    "# Get predictions and evaluate for Validation\n",
    "##############################################\n",
    "val_sorted_indices = get_predictions(df_val_input, doc_embeds, model, tokenizer)\n",
    "val_map25 = map_at_k(val_target_ids, val_sorted_indices, k=25)\n",
    "print(f\"Validation MAP@25: {val_map25:.4f}\")\n",
    "\n",
    "##############################################\n",
    "# Finally, produce submission for Test Set\n",
    "##############################################\n",
    "# Set IS_SUBMISSION = True if needed or just run since we have df_test\n",
    "test_sorted_indices = get_predictions(df_test_input, doc_embeds, model, tokenizer)\n",
    "\n",
    "df_test_input[\"MisconceptionId\"] = [\" \".join([str(x) for x in row]) for row in test_sorted_indices]\n",
    "df_test_input[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
