{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab7e905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:25.468571Z",
     "iopub.status.busy": "2024-11-26T06:40:25.468225Z",
     "iopub.status.idle": "2024-11-26T06:40:25.472228Z",
     "shell.execute_reply": "2024-11-26T06:40:25.471669Z"
    },
    "papermill": {
     "duration": 0.012441,
     "end_time": "2024-11-26T06:40:25.473852",
     "exception": false,
     "start_time": "2024-11-26T06:40:25.461411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !md5sum /kaggle/input/qw14b-awq/transformers/default/1/model-00001-of-00002.safetensors\n",
    "# should be 25596de367acaec20e616b7c87bd5529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffe4cf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:25.484603Z",
     "iopub.status.busy": "2024-11-26T06:40:25.484363Z",
     "iopub.status.idle": "2024-11-26T06:40:40.585053Z",
     "shell.execute_reply": "2024-11-26T06:40:40.583757Z"
    },
    "papermill": {
     "duration": 15.108336,
     "end_time": "2024-11-26T06:40:40.587332",
     "exception": false,
     "start_time": "2024-11-26T06:40:25.478996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install /kaggle/input/eedi-library/autoawq*.whl /kaggle/input/eedi-library/peft-0.13.2-py3-none-any.whl  --no-index --find-links=/kaggle/input/eedi-library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33344deb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:40.601466Z",
     "iopub.status.busy": "2024-11-26T06:40:40.601109Z",
     "iopub.status.idle": "2024-11-26T06:40:46.446886Z",
     "shell.execute_reply": "2024-11-26T06:40:46.446158Z"
    },
    "papermill": {
     "duration": 5.855569,
     "end_time": "2024-11-26T06:40:46.448972",
     "exception": false,
     "start_time": "2024-11-26T06:40:40.593403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hc4293/miniconda3/envs/nlpenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, math, numpy as np\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re, gc\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "pd.set_option('display.max_rows', 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a47be5dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:46.462334Z",
     "iopub.status.busy": "2024-11-26T06:40:46.461902Z",
     "iopub.status.idle": "2024-11-26T06:40:46.568132Z",
     "shell.execute_reply": "2024-11-26T06:40:46.567340Z"
    },
    "papermill": {
     "duration": 0.114514,
     "end_time": "2024-11-26T06:40:46.569957",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.455443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SUBMISSION: True\n"
     ]
    }
   ],
   "source": [
    "IS_SUBMISSION = True\n",
    "#bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "\n",
    "\n",
    "print('IS_SUBMISSION:', IS_SUBMISSION)\n",
    "\n",
    "model_path = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "df_train = pd.read_csv(\"./data/train.csv\").fillna(-1).sample(10, random_state=42).reset_index(drop=True)\n",
    "df_test = pd.read_csv(\"./data/test.csv\")\n",
    "df_misconception_mapping = pd.read_csv(\"./data/misconception_mapping.csv\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7003ff",
   "metadata": {
    "papermill": {
     "duration": 0.005429,
     "end_time": "2024-11-26T06:40:46.581149",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.575720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# first retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c6a754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:46.593511Z",
     "iopub.status.busy": "2024-11-26T06:40:46.593203Z",
     "iopub.status.idle": "2024-11-26T06:40:46.597343Z",
     "shell.execute_reply": "2024-11-26T06:40:46.596554Z"
    },
    "papermill": {
     "duration": 0.012276,
     "end_time": "2024-11-26T06:40:46.598960",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.586684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "if not IS_SUBMISSION:\n",
    "    df_ret = df_train.copy()\n",
    "else:\n",
    "    df_ret = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bfaaade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:46.610830Z",
     "iopub.status.busy": "2024-11-26T06:40:46.610580Z",
     "iopub.status.idle": "2024-11-26T06:40:46.628449Z",
     "shell.execute_reply": "2024-11-26T06:40:46.627655Z"
    },
    "papermill": {
     "duration": 0.02564,
     "end_time": "2024-11-26T06:40:46.630006",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.604366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>ConstructId</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectId</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>AnswerAText</th>\n",
       "      <th>AnswerBText</th>\n",
       "      <th>AnswerCText</th>\n",
       "      <th>AnswerDText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>D</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>Does not simplify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>B</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nTom</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "      <td>Neither is correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QuestionId  ConstructId                                      ConstructName  \\\n",
       "0        1869          856  Use the order of operations to carry out calcu...   \n",
       "1        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
       "2        1871         2774            Calculate the range from a list of data   \n",
       "\n",
       "   SubjectId                                        SubjectName CorrectAnswer  \\\n",
       "0         33                                             BIDMAS             A   \n",
       "1       1077                    Simplifying Algebraic Fractions             D   \n",
       "2        339  Range and Interquartile Range from a List of Data             B   \n",
       "\n",
       "                                        QuestionText            AnswerAText  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
       "2  Tom and Katie are discussing the \\( 5 \\) plant...              Only\\nTom   \n",
       "\n",
       "              AnswerBText            AnswerCText             AnswerDText  \n",
       "0  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets  \n",
       "1               \\( m+2 \\)              \\( m-1 \\)       Does not simplify  \n",
       "2             Only\\nKatie     Both Tom and Katie      Neither is correct  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81e283fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:46.642342Z",
     "iopub.status.busy": "2024-11-26T06:40:46.642038Z",
     "iopub.status.idle": "2024-11-26T06:40:46.650644Z",
     "shell.execute_reply": "2024-11-26T06:40:46.649842Z"
    },
    "papermill": {
     "duration": 0.016692,
     "end_time": "2024-11-26T06:40:46.652247",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.635555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMPLATE_INPUT_V3 = '{QUESTION}\\nCorrect answer: {CORRECT_ANSWER}\\nStudent wrong answer: {STUDENT_WRONG_ANSWER}'\n",
    "def format_input_v3(row, wrong_choice):\n",
    "\n",
    "    assert wrong_choice in \"ABCD\"\n",
    "    # Extract values from the row\n",
    "    question_text = row.get(\"QuestionText\", \"No question text provided\")\n",
    "    subject_name = row.get(\"SubjectName\", \"Unknown subject\")\n",
    "    construct_name = row.get(\"ConstructName\", \"Unknown construct\")\n",
    "    # Extract the correct and wrong answer text based on the choice\n",
    "    correct_answer = row.get(\"CorrectAnswer\", \"Unknown\")\n",
    "    assert wrong_choice != correct_answer\n",
    "    correct_answer_text = row.get(f\"Answer{correct_answer}Text\", \"No correct answer text available\")\n",
    "    wrong_answer_text = row.get(f\"Answer{wrong_choice}Text\", \"No wrong answer text available\")\n",
    "\n",
    "    # Construct the question format\n",
    "    formatted_question = f\"\"\"Question: {question_text}\n",
    "    \n",
    "SubjectName: {subject_name}\n",
    "ConstructName: {construct_name}\"\"\"\n",
    "\n",
    "    # Return the extracted data\n",
    "    ret = {\n",
    "        \"QUESTION\": formatted_question,\n",
    "        \"CORRECT_ANSWER\": correct_answer_text,\n",
    "        \"STUDENT_WRONG_ANSWER\": wrong_answer_text,\n",
    "        \"MISCONCEPTION_ID\": row.get('Misconception{wrong_choice}Id'),\n",
    "    }\n",
    "    ret[\"PROMPT\"] = TEMPLATE_INPUT_V3.format(**ret)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "items = []\n",
    "target_ids = []\n",
    "for _, row in df_ret.iterrows():\n",
    "    for choice in ['A', 'B', 'C', 'D']:\n",
    "        if choice == row[\"CorrectAnswer\"]:\n",
    "            continue\n",
    "        if not IS_SUBMISSION and row[f'Misconception{choice}Id'] == -1:\n",
    "            continue\n",
    "            \n",
    "        correct_col = f\"Answer{row['CorrectAnswer']}Text\"\n",
    "        item = {'QuestionId_Answer': '{}_{}'.format(row['QuestionId'], choice)}\n",
    "        item['Prompt'] = format_input_v3(row, choice)['PROMPT']\n",
    "        items.append(item)\n",
    "        target_ids.append(int(row.get(f'Misconception{choice}Id', -1)))\n",
    "        \n",
    "df_input = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0270597a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:46.664315Z",
     "iopub.status.busy": "2024-11-26T06:40:46.664058Z",
     "iopub.status.idle": "2024-11-26T06:41:00.375562Z",
     "shell.execute_reply": "2024-11-26T06:41:00.374794Z"
    },
    "papermill": {
     "duration": 13.72003,
     "end_time": "2024-11-26T06:41:00.377781",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.657751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}'\n",
    "\n",
    "def get_detailed_example(task_description: str, query: str, response: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}\\n<response>{response}'\n",
    "\n",
    "def get_new_queries(queries, query_max_len, examples_prefix, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        queries,\n",
    "        max_length=query_max_len - len(tokenizer('<s>', add_special_tokens=False)['input_ids']) - len(\n",
    "            tokenizer('\\n<response></s>', add_special_tokens=False)['input_ids']),\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    prefix_ids = tokenizer(examples_prefix, add_special_tokens=False)['input_ids']\n",
    "    suffix_ids = tokenizer('\\n<response>', add_special_tokens=False)['input_ids']\n",
    "    new_max_length = (len(prefix_ids) + len(suffix_ids) + query_max_len + 8) // 8 * 8 + 8\n",
    "    new_queries = tokenizer.batch_decode(inputs['input_ids'])\n",
    "    for i in range(len(new_queries)):\n",
    "        new_queries[i] = examples_prefix + new_queries[i] + '\\n<response>'\n",
    "    return new_max_length, new_queries\n",
    "task =  \"Given a math multiple-choice problem with a student's wrong answer, retrieve the math misconceptions\"\n",
    "queries = [\n",
    "    get_detailed_instruct(task, q) for q in df_input['Prompt']\n",
    "]\n",
    "documents = df_misconception_mapping['MisconceptionName'].tolist()\n",
    "query_max_len, doc_max_len = 320, 48\n",
    "LORA_PATH = \"saved_models/lora-14b-1126\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)\n",
    "examples_prefix = ''\n",
    "new_query_max_len, new_queries = get_new_queries(queries, query_max_len, examples_prefix, tokenizer)\n",
    "\n",
    "\n",
    "import json\n",
    "with open('data.json', 'w') as f:\n",
    "    data = {'texts': new_queries+ documents}\n",
    "    f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bab7a4dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:41:00.391099Z",
     "iopub.status.busy": "2024-11-26T06:41:00.390570Z",
     "iopub.status.idle": "2024-11-26T06:41:00.397726Z",
     "shell.execute_reply": "2024-11-26T06:41:00.396908Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.015465,
     "end_time": "2024-11-26T06:41:00.399194",
     "exception": false,
     "start_time": "2024-11-26T06:41:00.383729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_embed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_embed.py\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import peft\n",
    "\n",
    "MAX_LENGTH = 320\n",
    "\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[\n",
    "            torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_embeddings_in_batches(model, tokenizer, texts, max_length, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_dict = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(**batch_dict)\n",
    "            batch_embeddings = last_token_pool(\n",
    "                outputs.last_hidden_state, batch_dict[\"attention_mask\"]\n",
    "            )\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1).cpu()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(base_model_path, lora_path, load_in_4bit=True):\n",
    "    model = AutoModel.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map=0,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        lora_path if lora_path else base_model_path\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if lora_path:\n",
    "        model = peft.PeftModel.from_pretrained(model, lora_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    output_file = args.input_text.replace(\n",
    "        \".json\", \".pt.fold.{}.{}.embed\".format(*args.fold)\n",
    "    )\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Output file {output_file} already exists. Skipping...\")\n",
    "        return\n",
    "    model, tokenizer = load_model_and_tokenizer(\n",
    "        args.base_model, args.lora_path, load_in_4bit=args.load_in_4bit\n",
    "    )\n",
    "    texts = json.load(open(args.input_text))[\"texts\"][args.fold[0] :: args.fold[1]]\n",
    "    embeddings = get_embeddings_in_batches(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        batch_size=4,\n",
    "    )\n",
    "    text2embeds = {text: emb for text, emb in zip(texts, embeddings)}\n",
    "    torch.save(text2embeds, output_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--base_model\",\n",
    "        type=str,\n",
    "        default=\"Qwen/Qwen2.5-7B\",\n",
    "        help=\"Path to the base model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to the LoRA model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_text\",\n",
    "        type=str,\n",
    "        default=\".cache/data.json\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_in_4bit\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Load model in 4-bit mode\",\n",
    "    )\n",
    "    parser.add_argument(\"--fold\", nargs=2, type=int, default=[0, 1])\n",
    "    args = parser.parse_args()\n",
    "    if not os.path.exists(args.lora_path):\n",
    "        args.lora_path = None\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757ffb76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:41:00.411368Z",
     "iopub.status.busy": "2024-11-26T06:41:00.411085Z",
     "iopub.status.idle": "2024-11-26T06:41:04.427469Z",
     "shell.execute_reply": "2024-11-26T06:41:04.426579Z"
    },
    "papermill": {
     "duration": 4.024763,
     "end_time": "2024-11-26T06:41:04.429608",
     "exception": false,
     "start_time": "2024-11-26T06:41:00.404845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!sleep 1 & sleep 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b968fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:41:04.443192Z",
     "iopub.status.busy": "2024-11-26T06:41:04.442879Z",
     "iopub.status.idle": "2024-11-26T06:46:40.050083Z",
     "shell.execute_reply": "2024-11-26T06:46:40.049180Z"
    },
    "papermill": {
     "duration": 335.618424,
     "end_time": "2024-11-26T06:46:40.054442",
     "exception": false,
     "start_time": "2024-11-26T06:41:04.436018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
      "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp-a34b3233.so.1 library.\n",
      "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%writefile run.sh\n",
    "lora_path = '/kaggle/input/2211-lora-14b/transformers/default/1'\n",
    "cmd = f\"(CUDA_VISIBLE_DEVICES=0 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 0 2) & (CUDA_VISIBLE_DEVICES=1 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 1 2)\"\n",
    "import os\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d39800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:46:40.111043Z",
     "iopub.status.busy": "2024-11-26T06:46:40.110101Z",
     "iopub.status.idle": "2024-11-26T06:47:09.238653Z",
     "shell.execute_reply": "2024-11-26T06:47:09.237334Z"
    },
    "papermill": {
     "duration": 29.156178,
     "end_time": "2024-11-26T06:47:09.241638",
     "exception": false,
     "start_time": "2024-11-26T06:46:40.085460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m files \u001b[38;5;241m=\u001b[39m glob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.pt*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m     files \u001b[38;5;241m=\u001b[39m glob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.pt*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)    \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import time\n",
    "text_to_embed = {}\n",
    "files = glob('*.pt*')\n",
    "while len(files) != 2:\n",
    "    time.sleep(1)\n",
    "    files = glob('*.pt*')\n",
    "\n",
    "\n",
    "time.sleep(3)    \n",
    "for path in files:\n",
    "    print(path)\n",
    "    text_to_embed.update(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82641137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.294697Z",
     "iopub.status.busy": "2024-11-26T06:47:09.294397Z",
     "iopub.status.idle": "2024-11-26T06:47:09.299556Z",
     "shell.execute_reply": "2024-11-26T06:47:09.298770Z"
    },
    "papermill": {
     "duration": 0.032492,
     "end_time": "2024-11-26T06:47:09.301173",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.268681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2596"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d26804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.351687Z",
     "iopub.status.busy": "2024-11-26T06:47:09.351426Z",
     "iopub.status.idle": "2024-11-26T06:47:09.354817Z",
     "shell.execute_reply": "2024-11-26T06:47:09.354165Z"
    },
    "papermill": {
     "duration": 0.030277,
     "end_time": "2024-11-26T06:47:09.356358",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.326081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [text_to_embed[t] for t in new_queries][0.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22317209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.406615Z",
     "iopub.status.busy": "2024-11-26T06:47:09.406353Z",
     "iopub.status.idle": "2024-11-26T06:47:09.456901Z",
     "shell.execute_reply": "2024-11-26T06:47:09.456080Z"
    },
    "papermill": {
     "duration": 0.077719,
     "end_time": "2024-11-26T06:47:09.458586",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.380867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 5120]), torch.Size([2587, 5120]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings = torch.stack([text_to_embed[t] for t in new_queries])\n",
    "doc_embeddings = torch.stack([text_to_embed[t] for t in documents])\n",
    "query_embeddings.shape, doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e46302",
   "metadata": {
    "papermill": {
     "duration": 0.02453,
     "end_time": "2024-11-26T06:47:09.508213",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.483683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Eval (testing purpose only)\n",
    "This will not be ran in real submition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8fc60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.559287Z",
     "iopub.status.busy": "2024-11-26T06:47:09.558958Z",
     "iopub.status.idle": "2024-11-26T06:47:09.566099Z",
     "shell.execute_reply": "2024-11-26T06:47:09.565396Z"
    },
    "papermill": {
     "duration": 0.034535,
     "end_time": "2024-11-26T06:47:09.567689",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.533154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "def compute_metrics(q_embeds: torch.Tensor, d_embeds: torch.Tensor, target_ids: List[int]):\n",
    "    \"\"\"\n",
    "    Compute MAP@25 and Recall@100 metrics.\n",
    "    \n",
    "    Args:\n",
    "        q_embeds (torch.Tensor): Query embeddings of shape (M, dim), where M is the number of queries.\n",
    "        d_embeds (torch.Tensor): Document embeddings of shape (N, dim), where N is the number of documents.\n",
    "        target_ids (List[int]): List of target document indices (length M, one target index per query).\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints MAP@25 and Recall@100.\n",
    "    \"\"\"\n",
    "    # Compute similarity scores\n",
    "    scores = q_embeds @ d_embeds.T  # Shape: (M, N)\n",
    "\n",
    "    # Initialize variables for metrics\n",
    "    avg_precisions = []  # To store average precision for each query\n",
    "    recall_counts = []   # To store recall@100 counts for each query\n",
    "\n",
    "    # Compute metrics for each query\n",
    "    for i, target_id in enumerate(target_ids):\n",
    "        # Sort document indices by score in descending order\n",
    "        sorted_indices = torch.argsort(scores[i], descending=True)\n",
    "\n",
    "        # Compute precision@k and recall@100\n",
    "        relevant_docs = (sorted_indices[:100] == target_id).nonzero(as_tuple=True)[0]  # Find rank within top 100\n",
    "        recall_count = 1 if len(relevant_docs) > 0 else 0  # Check if target is in the top 100\n",
    "        recall_counts.append(recall_count)\n",
    "\n",
    "        # Compute average precision for top 25 (MAP@25)\n",
    "        precision_at_k = 0.0\n",
    "        num_relevant = 0\n",
    "        for rank, idx in enumerate(sorted_indices[:25]):\n",
    "            if idx == target_id:\n",
    "                num_relevant += 1\n",
    "                precision_at_k += num_relevant / (rank + 1)\n",
    "        avg_precisions.append(precision_at_k / 1 if num_relevant > 0 else 0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    map25 = sum(avg_precisions) / len(avg_precisions)\n",
    "    recall100 = sum(recall_counts) / len(recall_counts)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"MAP@25: {map25:.4f}\")\n",
    "    print(f\"Recall@100: {recall100:.4f}\")\n",
    "if not IS_SUBMISSION:\n",
    "    compute_metrics(query_embeddings, doc_embeddings, target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e124d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.617917Z",
     "iopub.status.busy": "2024-11-26T06:47:09.617669Z",
     "iopub.status.idle": "2024-11-26T06:47:09.685067Z",
     "shell.execute_reply": "2024-11-26T06:47:09.684324Z"
    },
    "papermill": {
     "duration": 0.094603,
     "end_time": "2024-11-26T06:47:09.686901",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.592298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = query_embeddings @ doc_embeddings.T  # Shape: (M, N)\n",
    "sorted_indices = torch.argsort(scores,1, descending=True)[:,:25].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fed996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.738675Z",
     "iopub.status.busy": "2024-11-26T06:47:09.738404Z",
     "iopub.status.idle": "2024-11-26T06:47:09.760557Z",
     "shell.execute_reply": "2024-11-26T06:47:09.759737Z"
    },
    "papermill": {
     "duration": 0.049973,
     "end_time": "2024-11-26T06:47:09.762352",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.712379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>706 1507 1345 2306 328 1672 1005 2518 1963 253...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>2306 1507 706 1005 1345 1999 2488 2532 987 251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>1005 328 1507 2532 1672 1516 706 1345 2306 248...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>2142 2068 167 418 891 1755 979 113 1421 2256 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>2142 2068 167 891 341 979 1755 1871 143 418 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>2142 2068 167 891 1755 418 113 2078 143 979 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1287 1073 2439 1665 2551 1306 1059 1098 1677 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1287 1073 2439 1665 2551 1098 1059 912 1306 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1073 1287 1059 1866 903 2471 912 2439 2064 167...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  706 1507 1345 2306 328 1672 1005 2518 1963 253...\n",
       "1            1869_C  2306 1507 706 1005 1345 1999 2488 2532 987 251...\n",
       "2            1869_D  1005 328 1507 2532 1672 1516 706 1345 2306 248...\n",
       "3            1870_A  2142 2068 167 418 891 1755 979 113 1421 2256 3...\n",
       "4            1870_B  2142 2068 167 891 341 979 1755 1871 143 418 11...\n",
       "5            1870_C  2142 2068 167 891 1755 418 113 2078 143 979 26...\n",
       "6            1871_A  1287 1073 2439 1665 2551 1306 1059 1098 1677 1...\n",
       "7            1871_C  1287 1073 2439 1665 2551 1098 1059 912 1306 16...\n",
       "8            1871_D  1073 1287 1059 1866 903 2471 912 2439 2064 167..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input[\"MisconceptionId\"] = [\" \".join([str(x) for x in row]) for row in sorted_indices]\n",
    "df_input[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b35c1",
   "metadata": {
    "papermill": {
     "duration": 0.024334,
     "end_time": "2024-11-26T06:47:09.944965",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.920631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07e3ea",
   "metadata": {
    "papermill": {
     "duration": 0.024298,
     "end_time": "2024-11-26T06:47:09.993711",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.969413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f16a3",
   "metadata": {
    "papermill": {
     "duration": 0.024939,
     "end_time": "2024-11-26T06:47:10.043044",
     "exception": false,
     "start_time": "2024-11-26T06:47:10.018105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c614db",
   "metadata": {
    "papermill": {
     "duration": 0.025352,
     "end_time": "2024-11-26T06:47:10.093816",
     "exception": false,
     "start_time": "2024-11-26T06:47:10.068464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10009171,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 209651782,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 169032,
     "modelInstanceId": 146504,
     "sourceId": 172110,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171421,
     "modelInstanceId": 148911,
     "sourceId": 174909,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171434,
     "modelInstanceId": 148923,
     "sourceId": 174921,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171497,
     "modelInstanceId": 148996,
     "sourceId": 175000,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 174732,
     "modelInstanceId": 152276,
     "sourceId": 178762,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 409.898595,
   "end_time": "2024-11-26T06:47:12.673711",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-26T06:40:22.775116",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
