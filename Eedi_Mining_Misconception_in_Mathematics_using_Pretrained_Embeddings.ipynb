{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6-lyPRC97dy",
        "outputId": "b2d7b458-588d-4e5a-8d2e-ac10e6557a74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-12 03:46:21--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-12-12 03:46:22--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-12-12 03:46:22--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 40s  \n",
            "\n",
            "2024-12-12 03:49:04 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download FastText embeddings\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "\n",
        "# Extract the compressed file\n",
        "!gunzip cc.en.300.vec.gz\n",
        "\n",
        "print(\"FastText embeddings downloaded and extracted successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8ltaOv9-EZL",
        "outputId": "b961db11-1625-41df-f661-688584f2a8be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-12 03:49:32--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.169.121.107, 3.169.121.57, 3.169.121.110, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.169.121.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G   138MB/s    in 12s     \n",
            "\n",
            "2024-12-12 03:49:44 (107 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n",
            "FastText embeddings downloaded and extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download leadbest/googlenewsvectorsnegative300\n",
        "!unzip -q googlenewsvectorsnegative300.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IystYCjE-I_A",
        "outputId": "4f165e19-5965-4710-c846-6b6543958040"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\n",
            "License(s): other\n",
            "Downloading googlenewsvectorsnegative300.zip to /content\n",
            "100% 3.17G/3.17G [02:19<00:00, 25.7MB/s]\n",
            "100% 3.17G/3.17G [02:19<00:00, 24.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "metadata": {
        "id": "LPnYVAKTROcH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "misconception_mapping_df = pd.read_csv('misconception_mapping.csv')"
      ],
      "metadata": {
        "id": "omJI8h7nRR3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add context to each question-answer pair for train data\n",
        "for answer in ['A', 'B', 'C', 'D']:\n",
        "    train_df[f'QA_{answer}'] = train_df['ConstructName'] + \" \" + train_df['SubjectName'] + \" \" + train_df['QuestionText'] + \" \" + train_df[f'Answer{answer}Text']\n",
        "\n",
        "# Stack question-answer pairs into a single DataFrame with corresponding misconception IDs\n",
        "qa_pairs = pd.DataFrame({\n",
        "    'QA_Text': pd.concat([train_df[f'QA_{answer}'] for answer in ['A', 'B', 'C', 'D']], axis=0),\n",
        "    'MisconceptionId': pd.concat([train_df[f'Misconception{answer}Id'] for answer in ['A', 'B', 'C', 'D']], axis=0),\n",
        "}).dropna()\n",
        "qa_pairs['MisconceptionId'] = qa_pairs['MisconceptionId'].astype(int)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(qa_pairs['QA_Text'])"
      ],
      "metadata": {
        "id": "bj1BAPk8RgfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained embeddings\n",
        "# Convert GloVe to Word2Vec format\n",
        "glove_input_file = \"glove.6B.100d.txt\"\n",
        "word2vec_output_file = \"glove.6B.100d.word2vec\"\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
        "fasttext_model = KeyedVectors.load_word2vec_format(\"cc.en.300.vec\", binary=False)\n",
        "word2vec_model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
      ],
      "metadata": {
        "id": "Pd3Z1M_7RkYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute sentence embeddings by averaging word vectors\n",
        "def sentence_to_embedding(sentence, embedding_model, embedding_dim=100):\n",
        "    words = sentence.split()\n",
        "    vectors = [embedding_model[word] for word in words if word in embedding_model]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(embedding_dim)\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = {\n",
        "    \"GloVe\": np.array([sentence_to_embedding(text, glove_model, 100) for text in qa_pairs['QA_Text']]),\n",
        "    \"FastText\": np.array([sentence_to_embedding(text, fasttext_model, 300) for text in qa_pairs['QA_Text']]),\n",
        "    \"Word2Vec\": np.array([sentence_to_embedding(text, word2vec_model, 300) for text in qa_pairs['QA_Text']]),\n",
        "}\n",
        "\n",
        "\n",
        "# Function to calculate MAP@K\n",
        "def map_at_k(y_true, y_pred, k=25):\n",
        "    average_precisions = []\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        relevance = np.isin(pred[:k], [true])\n",
        "        precision_at_k = [np.mean(relevance[:i+1]) for i in range(len(relevance)) if relevance[i]]\n",
        "        if precision_at_k:\n",
        "            average_precisions.append(np.mean(precision_at_k))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    return np.mean(average_precisions)"
      ],
      "metadata": {
        "id": "94iesA-4Rojy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate MAP@25 for each embedding\n",
        "embedding_scores = {}\n",
        "for name, embedding in embeddings.items():\n",
        "    single_embedding_features = np.hstack([tfidf_features.toarray(), embedding])\n",
        "    single_lsa_features = TruncatedSVD(n_components=100, random_state=42).fit_transform(single_embedding_features)\n",
        "    X_train_single, X_val_single, y_train_single, y_val_single = train_test_split(single_lsa_features, qa_pairs['MisconceptionId'], test_size=0.2, random_state=42)\n",
        "    cosine_sim_matrix_single = cosine_similarity(X_val_single, X_train_single)\n",
        "    top_25_preds_single = np.argsort(cosine_sim_matrix_single, axis=1)[:, -25:][:, ::-1]\n",
        "    embedding_scores[name] = map_at_k(\n",
        "        y_val_single.values,\n",
        "        [[y_train_single.iloc[i] for i in indices] for indices in top_25_preds_single]\n",
        "    )\n",
        "\n",
        "for name, score in embedding_scores.items():\n",
        "    print(f\"MAP@25 Score with {name} Embedding: {score}\")\n",
        "\n",
        "# Combine embeddings\n",
        "combined_embeddings = np.hstack([embeddings[\"GloVe\"], embeddings[\"FastText\"], embeddings[\"Word2Vec\"]])\n",
        "\n",
        "# Combine with TF-IDF features\n",
        "combined_features = np.hstack([tfidf_features.toarray(), combined_embeddings])\n",
        "\n",
        "# Dimensionality reduction with SVD\n",
        "svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "lsa_features = svd.fit_transform(combined_features)\n",
        "\n",
        "# Train-validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(lsa_features, qa_pairs['MisconceptionId'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute cosine similarity for validation set\n",
        "cosine_sim_matrix = cosine_similarity(X_val, X_train)\n",
        "top_25_preds = np.argsort(cosine_sim_matrix, axis=1)[:, -25:][:, ::-1]"
      ],
      "metadata": {
        "id": "5avT4tfhRu7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate MAP@25\n",
        "map25_score = map_at_k(\n",
        "    y_val.values,\n",
        "    [[y_train.iloc[i] for i in indices] for indices in top_25_preds]\n",
        ")\n",
        "print(f\"MAP@25 Score with Combined Embeddings: {map25_score}\")\n",
        "\n",
        "# Preprocess test data\n",
        "for answer in ['A', 'B', 'C', 'D']:\n",
        "    test_df[f'QA_{answer}'] = test_df['ConstructName'] + \" \" + test_df['SubjectName'] + \" \" + test_df['QuestionText'] + \" \" + test_df[f'Answer{answer}Text']\n",
        "\n",
        "# Stack test question-answer pairs, excluding the correct answer\n",
        "test_qa_pairs = []\n",
        "for _, row in test_df.iterrows():\n",
        "    correct_answer = row['CorrectAnswer']\n",
        "    for answer in ['A', 'B', 'C', 'D']:\n",
        "        if answer != correct_answer:\n",
        "            test_qa_pairs.append({\n",
        "                'QuestionId_Answer': f\"{row['QuestionId']}_{answer}\",\n",
        "                'QA_Text': row[f'QA_{answer}']\n",
        "            })\n",
        "\n",
        "test_qa_pairs = pd.DataFrame(test_qa_pairs)\n",
        "\n",
        "# Generate embeddings for test data\n",
        "test_embeddings = {\n",
        "    \"GloVe\": np.array([sentence_to_embedding(text, glove_model, 100) for text in test_qa_pairs['QA_Text']]),\n",
        "    \"FastText\": np.array([sentence_to_embedding(text, fasttext_model, 300) for text in test_qa_pairs['QA_Text']]),\n",
        "    \"Word2Vec\": np.array([sentence_to_embedding(text, word2vec_model, 300) for text in test_qa_pairs['QA_Text']]),\n",
        "}\n",
        "combined_test_embeddings = np.hstack([test_embeddings[\"GloVe\"], test_embeddings[\"FastText\"], test_embeddings[\"Word2Vec\"]])\n",
        "\n",
        "# Combine with TF-IDF features for test data\n",
        "test_tfidf = tfidf_vectorizer.transform(test_qa_pairs['QA_Text'])\n",
        "test_combined_features = np.hstack([test_tfidf.toarray(), combined_test_embeddings])\n",
        "\n",
        "# Dimensionality reduction for test data\n",
        "test_lsa_features = svd.transform(test_combined_features)\n",
        "\n",
        "# Compute cosine similarity between test and training data\n",
        "test_cosine_sim_matrix = cosine_similarity(test_lsa_features, X_train)\n",
        "top_k_test_preds = np.argsort(test_cosine_sim_matrix, axis=1)[:, -25:][:, ::-1]\n",
        "\n",
        "# Format predictions for submission\n",
        "submission = pd.DataFrame({\n",
        "    'QuestionId_Answer': test_qa_pairs['QuestionId_Answer'],\n",
        "    'MisconceptionId': [' '.join(map(str, preds)) for preds in top_k_test_preds]\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv(\"submission_combined_embeddings.csv\", index=False)\n",
        "print(\"Submission file created successfully as submission_combined_embeddings.csv!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7fvv4WZ10NW",
        "outputId": "953b1331-c4b1-4c0d-ebbd-43dc002f3fd4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-3549d6ad8762>:34: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@25 Score with GloVe Embedding: 0.3328175587464839\n",
            "MAP@25 Score with FastText Embedding: 0.32924964639166243\n",
            "MAP@25 Score with Word2Vec Embedding: 0.32456491861729064\n",
            "MAP@25 Score with Combined Embeddings: 0.335783479062598\n",
            "Submission file created successfully as submission_combined_embeddings.csv!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data  = pd.read_csv('submission_combined_embeddings.csv')\n",
        "with pd.option_context('display.max_rows', None):\n",
        "    print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPm_Qcoh-Psz",
        "outputId": "68fc2592-7a89-4da5-87bd-0ac3d6d7cd92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  QuestionId_Answer                                    MisconceptionId\n",
            "0            1869_B  205 3419 1462 3044 2532 413 3246 1375 2944 235...\n",
            "1            1869_C  205 3419 1462 3044 2532 413 3246 1375 2944 235...\n",
            "2            1869_D  205 1462 1375 3044 413 2532 3419 2944 3246 235...\n",
            "3            1870_A  533 422 585 1029 368 1668 1250 385 34 1567 277...\n",
            "4            1870_B  533 422 585 1029 368 1668 1250 385 34 1567 277...\n",
            "5            1870_C  1668 422 533 585 1029 368 1250 385 34 1567 226...\n",
            "6            1871_A  1623 2185 3307 2313 3468 3326 1286 2896 616 14...\n",
            "7            1871_C  1623 2185 3307 2313 3468 3326 1286 142 616 289...\n",
            "8            1871_D  2185 1623 3307 2313 3468 3326 1286 2896 750 10...\n"
          ]
        }
      ]
    }
  ]
}