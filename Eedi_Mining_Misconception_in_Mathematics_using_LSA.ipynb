{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ieFqlj9mHHXo"
      },
      "outputs": [],
      "source": [
        "# Install the Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8MsllrKqHTUt"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "misconception_mapping_df = pd.read_csv('misconception_mapping.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Xz_6OL6WHbwM"
      },
      "outputs": [],
      "source": [
        "# Add context to each question-answer pair for train data\n",
        "train_df['QA_A'] = train_df['ConstructName'] + \" \" + train_df['SubjectName'] + \" \" + train_df['QuestionText'] + \" \" + train_df['AnswerAText']\n",
        "train_df['QA_B'] = train_df['ConstructName'] + \" \" + train_df['SubjectName'] + \" \" + train_df['QuestionText'] + \" \" + train_df['AnswerBText']\n",
        "train_df['QA_C'] = train_df['ConstructName'] + \" \" + train_df['SubjectName'] + \" \" + train_df['QuestionText'] + \" \" + train_df['AnswerCText']\n",
        "train_df['QA_D'] = train_df['ConstructName'] + \" \" + train_df['SubjectName'] + \" \" + train_df['QuestionText'] + \" \" + train_df['AnswerDText']\n",
        "\n",
        "# Stack question-answer pairs into a single DataFrame with corresponding misconception IDs\n",
        "qa_pairs = pd.DataFrame({\n",
        "    'QA_Text': pd.concat([train_df['QA_A'], train_df['QA_B'], train_df['QA_C'], train_df['QA_D']], axis=0),\n",
        "    'MisconceptionId': pd.concat([train_df['MisconceptionAId'], train_df['MisconceptionBId'], train_df['MisconceptionCId'], train_df['MisconceptionDId']], axis=0),\n",
        "}).dropna()\n",
        "\n",
        "qa_pairs['MisconceptionId'] = qa_pairs['MisconceptionId'].astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rjFGmzk3HfBK"
      },
      "outputs": [],
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_text = tfidf_vectorizer.fit_transform(qa_pairs['QA_Text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AP8KrufbH5AY"
      },
      "outputs": [],
      "source": [
        "# Function to calculate MAP@K (K=25 in this case)\n",
        "def map_at_k(y_true, y_pred, k=25):\n",
        "    \"\"\"Compute Mean Average Precision at K for each sample in y_true and y_pred.\"\"\"\n",
        "    average_precisions = []\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        relevance = np.isin(pred[:k], [true])\n",
        "        precision_at_k = [np.mean(relevance[:i+1]) for i in range(len(relevance)) if relevance[i]]\n",
        "        if precision_at_k:\n",
        "            average_precisions.append(np.mean(precision_at_k))\n",
        "        else:\n",
        "            average_precisions.append(0)\n",
        "    return np.mean(average_precisions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y0E5nm5CBK8W"
      },
      "outputs": [],
      "source": [
        "# Function to perform LSA with a specified number of components and calculate MAP@25\n",
        "def evaluate_lsa(n_components):\n",
        "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "    X_reduced = svd.fit_transform(X_text)\n",
        "\n",
        "    # Train-validation split for parameter tuning\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_reduced, qa_pairs['MisconceptionId'], test_size=0.2, random_state=42)\n",
        "\n",
        "    # Cosine similarity for validation\n",
        "    cosine_sim_matrix = cosine_similarity(X_val, X_train)\n",
        "    top_25_preds = np.argsort(cosine_sim_matrix, axis=1)[:, -25:][:, ::-1]\n",
        "\n",
        "    # Gather the top 25 predictions for each validation sample\n",
        "    y_pred_top_25 = [[y_train.iloc[i] for i in indices] for indices in top_25_preds]\n",
        "\n",
        "    # Calculate MAP@25 score for this number of components\n",
        "    map25_score = map_at_k(y_val.values, y_pred_top_25)\n",
        "    return map25_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLZTR1TOIEYy",
        "outputId": "9fae1812-a0d3-4bf0-a164-08dec7df1efd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAP@25 Score with 50 components: 0.32204467450380125\n",
            "MAP@25 Score with 100 components: 0.3176076813186543\n",
            "MAP@25 Score with 200 components: 0.3163163148694109\n",
            "MAP@25 Score with 300 components: 0.3193932327855233\n",
            "Best n_components: 50 with MAP@25 Score: 0.32204467450380125\n"
          ]
        }
      ],
      "source": [
        "# Tuning n_components\n",
        "components_range = [50, 100, 200, 300]\n",
        "best_score = 0\n",
        "best_components = 0\n",
        "\n",
        "for n in components_range:\n",
        "    map25_score = evaluate_lsa(n)\n",
        "    print(f\"MAP@25 Score with {n} components: {map25_score}\")\n",
        "    if map25_score > best_score:\n",
        "        best_score = map25_score\n",
        "        best_components = n\n",
        "\n",
        "print(f\"Best n_components: {best_components} with MAP@25 Score: {best_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YHjNJKVnIIUq"
      },
      "outputs": [],
      "source": [
        "# Apply best n_components on the entire dataset for final predictions\n",
        "svd = TruncatedSVD(n_components=best_components, random_state=42)\n",
        "X_reduced = svd.fit_transform(tfidf_vectorizer.fit_transform(qa_pairs['QA_Text']))\n",
        "\n",
        "# Recompute the misconception LSA embeddings\n",
        "misconception_tfidf = tfidf_vectorizer.transform(misconception_mapping_df['MisconceptionName'])\n",
        "misconception_lsa = svd.transform(misconception_tfidf)  # Recompute misconception_lsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pk-OQxsWILjM"
      },
      "outputs": [],
      "source": [
        "# Prepare the test data with context\n",
        "test_df['QA_A'] = test_df['ConstructName'] + \" \" + test_df['SubjectName'] + \" \" + test_df['QuestionText'] + \" \" + test_df['AnswerAText']\n",
        "test_df['QA_B'] = test_df['ConstructName'] + \" \" + test_df['SubjectName'] + \" \" + test_df['QuestionText'] + \" \" + test_df['AnswerBText']\n",
        "test_df['QA_C'] = test_df['ConstructName'] + \" \" + test_df['SubjectName'] + \" \" + test_df['QuestionText'] + \" \" + test_df['AnswerCText']\n",
        "test_df['QA_D'] = test_df['ConstructName'] + \" \" + test_df['SubjectName'] + \" \" + test_df['QuestionText'] + \" \" + test_df['AnswerDText']\n",
        "\n",
        "# Stack test question-answer pairs, excluding the correct answer\n",
        "qa_pairs = []\n",
        "for _, row in test_df.iterrows():\n",
        "    correct_answer = row['CorrectAnswer']\n",
        "    for answer in ['A', 'B', 'C', 'D']:\n",
        "        if answer != correct_answer:\n",
        "            qa_pairs.append({\n",
        "                'QuestionId_Answer': f\"{row['QuestionId']}_{answer}\",\n",
        "                'QA_Text': row[f'QA_{answer}']\n",
        "            })\n",
        "\n",
        "test_qa_pairs = pd.DataFrame(qa_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmSkm6-KIRHi",
        "outputId": "17a95c9a-bb19-4f8d-e257-ca5a560e3ff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission file created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Transform the test data\n",
        "test_tfidf = tfidf_vectorizer.transform(test_qa_pairs['QA_Text'])\n",
        "test_lsa = svd.transform(test_tfidf)\n",
        "\n",
        "# Cosine similarity between test QA pairs and misconceptions\n",
        "similarity_matrix_test = cosine_similarity(test_lsa, misconception_lsa)\n",
        "top_k = 25\n",
        "top_k_indices_test = np.argsort(similarity_matrix_test, axis=1)[:, -top_k:][:, ::-1]\n",
        "\n",
        "# Format the predictions for submission\n",
        "submission = pd.DataFrame({\n",
        "    'QuestionId_Answer': test_qa_pairs['QuestionId_Answer'],\n",
        "    'MisconceptionId': [' '.join(map(str, preds)) for preds in top_k_indices_test]\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Submission file created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiE2-rwH9vZp",
        "outputId": "153e5b2e-2fee-4f1e-ee9c-87156d575980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  QuestionId_Answer                                    MisconceptionId\n",
            "0            1869_B  842 2532 987 657 2518 1999 1338 1929 1672 1941...\n",
            "1            1869_C  842 2532 987 657 2518 1999 1338 1929 1672 1941...\n",
            "2            1869_D  842 657 1005 2532 987 2488 1999 2518 1338 1929...\n",
            "3            1870_A  979 1540 885 363 29 1825 623 1928 1305 112 80 ...\n",
            "4            1870_B  979 1540 885 363 29 1825 623 1928 1305 112 80 ...\n",
            "5            1870_C  979 1540 885 363 29 1825 623 1928 1305 112 80 ...\n",
            "6            1871_A  632 549 1200 2211 1059 2551 2471 2439 2243 192...\n",
            "7            1871_C  632 1200 549 2211 1059 2551 2439 2471 2243 192...\n",
            "8            1871_D  632 549 1200 2211 1059 2551 2471 2439 2243 192...\n"
          ]
        }
      ],
      "source": [
        "data  = pd.read_csv('submission.csv')\n",
        "with pd.option_context('display.max_rows', None):\n",
        "    print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00YknRgH90K_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlpenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
