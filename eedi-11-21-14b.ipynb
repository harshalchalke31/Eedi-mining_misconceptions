{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab7e905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:25.468571Z",
     "iopub.status.busy": "2024-11-26T06:40:25.468225Z",
     "iopub.status.idle": "2024-11-26T06:40:25.472228Z",
     "shell.execute_reply": "2024-11-26T06:40:25.471669Z"
    },
    "papermill": {
     "duration": 0.012441,
     "end_time": "2024-11-26T06:40:25.473852",
     "exception": false,
     "start_time": "2024-11-26T06:40:25.461411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !md5sum /kaggle/input/qw14b-awq/transformers/default/1/model-00001-of-00002.safetensors\n",
    "# should be 25596de367acaec20e616b7c87bd5529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffe4cf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:25.484603Z",
     "iopub.status.busy": "2024-11-26T06:40:25.484363Z",
     "iopub.status.idle": "2024-11-26T06:40:40.585053Z",
     "shell.execute_reply": "2024-11-26T06:40:40.583757Z"
    },
    "papermill": {
     "duration": 15.108336,
     "end_time": "2024-11-26T06:40:40.587332",
     "exception": false,
     "start_time": "2024-11-26T06:40:25.478996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/eedi-library\r\n",
      "Processing /kaggle/input/eedi-library/autoawq-0.2.7.post2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/eedi-library/peft-0.13.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (2.4.0)\r\n",
      "Processing /kaggle/input/eedi-library/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from autoawq==0.2.7.post2)\r\n",
      "Requirement already satisfied: transformers>=4.35.0 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (4.45.1)\r\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (0.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (4.12.2)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (0.34.2)\r\n",
      "Requirement already satisfied: datasets>=2.20 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (3.0.1)\r\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.7.post2) (0.23.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (6.0.2)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (4.66.4)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (0.4.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.13.2) (0.25.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (3.15.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (2.2.2)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (2.32.3)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.20->autoawq==0.2.7.post2) (2024.6.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.20->autoawq==0.2.7.post2) (3.9.5)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.13.2) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.2.0->autoawq==0.2.7.post2) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.2.0->autoawq==0.2.7.post2) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.2.0->autoawq==0.2.7.post2) (3.1.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.35.0->autoawq==0.2.7.post2) (2024.5.15)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.20->autoawq==0.2.7.post2) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.7.post2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.7.post2) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.7.post2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.20->autoawq==0.2.7.post2) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.2.0->autoawq==0.2.7.post2) (2.1.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20->autoawq==0.2.7.post2) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20->autoawq==0.2.7.post2) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.20->autoawq==0.2.7.post2) (2024.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.2.0->autoawq==0.2.7.post2) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq==0.2.7.post2) (1.16.0)\r\n",
      "Installing collected packages: triton, peft, autoawq\r\n",
      "Successfully installed autoawq-0.2.7.post2 peft-0.13.2 triton-3.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/eedi-library/autoawq*.whl /kaggle/input/eedi-library/peft-0.13.2-py3-none-any.whl  --no-index --find-links=/kaggle/input/eedi-library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33344deb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:40.601466Z",
     "iopub.status.busy": "2024-11-26T06:40:40.601109Z",
     "iopub.status.idle": "2024-11-26T06:40:46.446886Z",
     "shell.execute_reply": "2024-11-26T06:40:46.446158Z"
    },
    "papermill": {
     "duration": 5.855569,
     "end_time": "2024-11-26T06:40:46.448972",
     "exception": false,
     "start_time": "2024-11-26T06:40:40.593403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, math, numpy as np\n",
    "import sys\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re, gc\n",
    "import torch\n",
    "pd.set_option('display.max_rows', 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a47be5dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:46.462334Z",
     "iopub.status.busy": "2024-11-26T06:40:46.461902Z",
     "iopub.status.idle": "2024-11-26T06:40:46.568132Z",
     "shell.execute_reply": "2024-11-26T06:40:46.567340Z"
    },
    "papermill": {
     "duration": 0.114514,
     "end_time": "2024-11-26T06:40:46.569957",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.455443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SUBMISSION: True\n"
     ]
    }
   ],
   "source": [
    "IS_SUBMISSION = True\n",
    "#bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "\n",
    "\n",
    "print('IS_SUBMISSION:', IS_SUBMISSION)\n",
    "\n",
    "model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "df_train = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\").fillna(-1).sample(10, random_state=42).reset_index(drop=True)\n",
    "df_test = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\n",
    "df_misconception_mapping = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7003ff",
   "metadata": {
    "papermill": {
     "duration": 0.005429,
     "end_time": "2024-11-26T06:40:46.581149",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.575720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# first retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c6a754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:46.593511Z",
     "iopub.status.busy": "2024-11-26T06:40:46.593203Z",
     "iopub.status.idle": "2024-11-26T06:40:46.597343Z",
     "shell.execute_reply": "2024-11-26T06:40:46.596554Z"
    },
    "papermill": {
     "duration": 0.012276,
     "end_time": "2024-11-26T06:40:46.598960",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.586684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "if not IS_SUBMISSION:\n",
    "    df_ret = df_train.copy()\n",
    "else:\n",
    "    df_ret = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bfaaade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:46.610830Z",
     "iopub.status.busy": "2024-11-26T06:40:46.610580Z",
     "iopub.status.idle": "2024-11-26T06:40:46.628449Z",
     "shell.execute_reply": "2024-11-26T06:40:46.627655Z"
    },
    "papermill": {
     "duration": 0.02564,
     "end_time": "2024-11-26T06:40:46.630006",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.604366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>ConstructId</th>\n",
       "      <th>ConstructName</th>\n",
       "      <th>SubjectId</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>AnswerAText</th>\n",
       "      <th>AnswerBText</th>\n",
       "      <th>AnswerCText</th>\n",
       "      <th>AnswerDText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869</td>\n",
       "      <td>856</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "      <td>33</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>A</td>\n",
       "      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n",
       "      <td>\\( 3 \\times(2+4)-5 \\)</td>\n",
       "      <td>\\( 3 \\times 2+(4-5) \\)</td>\n",
       "      <td>\\( 3 \\times(2+4-5) \\)</td>\n",
       "      <td>Does not need brackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870</td>\n",
       "      <td>1612</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "      <td>1077</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>D</td>\n",
       "      <td>Simplify the following, if possible: \\( \\frac{...</td>\n",
       "      <td>\\( m+1 \\)</td>\n",
       "      <td>\\( m+2 \\)</td>\n",
       "      <td>\\( m-1 \\)</td>\n",
       "      <td>Does not simplify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871</td>\n",
       "      <td>2774</td>\n",
       "      <td>Calculate the range from a list of data</td>\n",
       "      <td>339</td>\n",
       "      <td>Range and Interquartile Range from a List of Data</td>\n",
       "      <td>B</td>\n",
       "      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n",
       "      <td>Only\\nTom</td>\n",
       "      <td>Only\\nKatie</td>\n",
       "      <td>Both Tom and Katie</td>\n",
       "      <td>Neither is correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QuestionId  ConstructId                                      ConstructName  \\\n",
       "0        1869          856  Use the order of operations to carry out calcu...   \n",
       "1        1870         1612  Simplify an algebraic fraction by factorising ...   \n",
       "2        1871         2774            Calculate the range from a list of data   \n",
       "\n",
       "   SubjectId                                        SubjectName CorrectAnswer  \\\n",
       "0         33                                             BIDMAS             A   \n",
       "1       1077                    Simplifying Algebraic Fractions             D   \n",
       "2        339  Range and Interquartile Range from a List of Data             B   \n",
       "\n",
       "                                        QuestionText            AnswerAText  \\\n",
       "0  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...  \\( 3 \\times(2+4)-5 \\)   \n",
       "1  Simplify the following, if possible: \\( \\frac{...              \\( m+1 \\)   \n",
       "2  Tom and Katie are discussing the \\( 5 \\) plant...              Only\\nTom   \n",
       "\n",
       "              AnswerBText            AnswerCText             AnswerDText  \n",
       "0  \\( 3 \\times 2+(4-5) \\)  \\( 3 \\times(2+4-5) \\)  Does not need brackets  \n",
       "1               \\( m+2 \\)              \\( m-1 \\)       Does not simplify  \n",
       "2             Only\\nKatie     Both Tom and Katie      Neither is correct  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81e283fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:46.642342Z",
     "iopub.status.busy": "2024-11-26T06:40:46.642038Z",
     "iopub.status.idle": "2024-11-26T06:40:46.650644Z",
     "shell.execute_reply": "2024-11-26T06:40:46.649842Z"
    },
    "papermill": {
     "duration": 0.016692,
     "end_time": "2024-11-26T06:40:46.652247",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.635555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMPLATE_INPUT_V3 = '{QUESTION}\\nCorrect answer: {CORRECT_ANSWER}\\nStudent wrong answer: {STUDENT_WRONG_ANSWER}'\n",
    "def format_input_v3(row, wrong_choice):\n",
    "\n",
    "    assert wrong_choice in \"ABCD\"\n",
    "    # Extract values from the row\n",
    "    question_text = row.get(\"QuestionText\", \"No question text provided\")\n",
    "    subject_name = row.get(\"SubjectName\", \"Unknown subject\")\n",
    "    construct_name = row.get(\"ConstructName\", \"Unknown construct\")\n",
    "    # Extract the correct and wrong answer text based on the choice\n",
    "    correct_answer = row.get(\"CorrectAnswer\", \"Unknown\")\n",
    "    assert wrong_choice != correct_answer\n",
    "    correct_answer_text = row.get(f\"Answer{correct_answer}Text\", \"No correct answer text available\")\n",
    "    wrong_answer_text = row.get(f\"Answer{wrong_choice}Text\", \"No wrong answer text available\")\n",
    "\n",
    "    # Construct the question format\n",
    "    formatted_question = f\"\"\"Question: {question_text}\n",
    "    \n",
    "SubjectName: {subject_name}\n",
    "ConstructName: {construct_name}\"\"\"\n",
    "\n",
    "    # Return the extracted data\n",
    "    ret = {\n",
    "        \"QUESTION\": formatted_question,\n",
    "        \"CORRECT_ANSWER\": correct_answer_text,\n",
    "        \"STUDENT_WRONG_ANSWER\": wrong_answer_text,\n",
    "        \"MISCONCEPTION_ID\": row.get('Misconception{wrong_choice}Id'),\n",
    "    }\n",
    "    ret[\"PROMPT\"] = TEMPLATE_INPUT_V3.format(**ret)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "items = []\n",
    "target_ids = []\n",
    "for _, row in df_ret.iterrows():\n",
    "    for choice in ['A', 'B', 'C', 'D']:\n",
    "        if choice == row[\"CorrectAnswer\"]:\n",
    "            continue\n",
    "        if not IS_SUBMISSION and row[f'Misconception{choice}Id'] == -1:\n",
    "            continue\n",
    "            \n",
    "        correct_col = f\"Answer{row['CorrectAnswer']}Text\"\n",
    "        item = {'QuestionId_Answer': '{}_{}'.format(row['QuestionId'], choice)}\n",
    "        item['Prompt'] = format_input_v3(row, choice)['PROMPT']\n",
    "        items.append(item)\n",
    "        target_ids.append(int(row.get(f'Misconception{choice}Id', -1)))\n",
    "        \n",
    "df_input = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0270597a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:40:46.664315Z",
     "iopub.status.busy": "2024-11-26T06:40:46.664058Z",
     "iopub.status.idle": "2024-11-26T06:41:00.375562Z",
     "shell.execute_reply": "2024-11-26T06:41:00.374794Z"
    },
    "papermill": {
     "duration": 13.72003,
     "end_time": "2024-11-26T06:41:00.377781",
     "exception": false,
     "start_time": "2024-11-26T06:40:46.657751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}'\n",
    "\n",
    "def get_detailed_example(task_description: str, query: str, response: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}\\n<response>{response}'\n",
    "\n",
    "def get_new_queries(queries, query_max_len, examples_prefix, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        queries,\n",
    "        max_length=query_max_len - len(tokenizer('<s>', add_special_tokens=False)['input_ids']) - len(\n",
    "            tokenizer('\\n<response></s>', add_special_tokens=False)['input_ids']),\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    prefix_ids = tokenizer(examples_prefix, add_special_tokens=False)['input_ids']\n",
    "    suffix_ids = tokenizer('\\n<response>', add_special_tokens=False)['input_ids']\n",
    "    new_max_length = (len(prefix_ids) + len(suffix_ids) + query_max_len + 8) // 8 * 8 + 8\n",
    "    new_queries = tokenizer.batch_decode(inputs['input_ids'])\n",
    "    for i in range(len(new_queries)):\n",
    "        new_queries[i] = examples_prefix + new_queries[i] + '\\n<response>'\n",
    "    return new_max_length, new_queries\n",
    "task =  \"Given a math multiple-choice problem with a student's wrong answer, retrieve the math misconceptions\"\n",
    "queries = [\n",
    "    get_detailed_instruct(task, q) for q in df_input['Prompt']\n",
    "]\n",
    "documents = df_misconception_mapping['MisconceptionName'].tolist()\n",
    "query_max_len, doc_max_len = 320, 48\n",
    "LORA_PATH = '/kaggle/input/lora-14b-1126/transformers/default/1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)\n",
    "examples_prefix = ''\n",
    "new_query_max_len, new_queries = get_new_queries(queries, query_max_len, examples_prefix, tokenizer)\n",
    "\n",
    "\n",
    "import json\n",
    "with open('data.json', 'w') as f:\n",
    "    data = {'texts': new_queries+ documents}\n",
    "    f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bab7a4dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:41:00.391099Z",
     "iopub.status.busy": "2024-11-26T06:41:00.390570Z",
     "iopub.status.idle": "2024-11-26T06:41:00.397726Z",
     "shell.execute_reply": "2024-11-26T06:41:00.396908Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.015465,
     "end_time": "2024-11-26T06:41:00.399194",
     "exception": false,
     "start_time": "2024-11-26T06:41:00.383729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_embed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_embed.py\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import peft\n",
    "\n",
    "MAX_LENGTH = 320\n",
    "\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[\n",
    "            torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_embeddings_in_batches(model, tokenizer, texts, max_length, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_dict = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(**batch_dict)\n",
    "            batch_embeddings = last_token_pool(\n",
    "                outputs.last_hidden_state, batch_dict[\"attention_mask\"]\n",
    "            )\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1).cpu()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(base_model_path, lora_path, load_in_4bit=True):\n",
    "    model = AutoModel.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map=0,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        lora_path if lora_path else base_model_path\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if lora_path:\n",
    "        model = peft.PeftModel.from_pretrained(model, lora_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    output_file = args.input_text.replace(\n",
    "        \".json\", \".pt.fold.{}.{}.embed\".format(*args.fold)\n",
    "    )\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Output file {output_file} already exists. Skipping...\")\n",
    "        return\n",
    "    model, tokenizer = load_model_and_tokenizer(\n",
    "        args.base_model, args.lora_path, load_in_4bit=args.load_in_4bit\n",
    "    )\n",
    "    texts = json.load(open(args.input_text))[\"texts\"][args.fold[0] :: args.fold[1]]\n",
    "    embeddings = get_embeddings_in_batches(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        batch_size=4,\n",
    "    )\n",
    "    text2embeds = {text: emb for text, emb in zip(texts, embeddings)}\n",
    "    torch.save(text2embeds, output_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--base_model\",\n",
    "        type=str,\n",
    "        default=\"Qwen/Qwen2.5-7B\",\n",
    "        help=\"Path to the base model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to the LoRA model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_text\",\n",
    "        type=str,\n",
    "        default=\".cache/data.json\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_in_4bit\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Load model in 4-bit mode\",\n",
    "    )\n",
    "    parser.add_argument(\"--fold\", nargs=2, type=int, default=[0, 1])\n",
    "    args = parser.parse_args()\n",
    "    if not os.path.exists(args.lora_path):\n",
    "        args.lora_path = None\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757ffb76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:41:00.411368Z",
     "iopub.status.busy": "2024-11-26T06:41:00.411085Z",
     "iopub.status.idle": "2024-11-26T06:41:04.427469Z",
     "shell.execute_reply": "2024-11-26T06:41:04.426579Z"
    },
    "papermill": {
     "duration": 4.024763,
     "end_time": "2024-11-26T06:41:04.429608",
     "exception": false,
     "start_time": "2024-11-26T06:41:00.404845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!sleep 1 & sleep 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b968fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:41:04.443192Z",
     "iopub.status.busy": "2024-11-26T06:41:04.442879Z",
     "iopub.status.idle": "2024-11-26T06:46:40.050083Z",
     "shell.execute_reply": "2024-11-26T06:46:40.049180Z"
    },
    "papermill": {
     "duration": 335.618424,
     "end_time": "2024-11-26T06:46:40.054442",
     "exception": false,
     "start_time": "2024-11-26T06:41:04.436018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:51<00:00, 55.64s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:51<00:00, 55.81s/it]\n",
      "Embedding: 100%|██████████| 325/325 [03:08<00:00,  1.72it/s]\n",
      "Embedding:  88%|████████▊ | 286/325 [03:10<00:30,  1.30it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%writefile run.sh\n",
    "lora_path = '/kaggle/input/2211-lora-14b/transformers/default/1'\n",
    "cmd = f\"(CUDA_VISIBLE_DEVICES=0 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 0 2) & (CUDA_VISIBLE_DEVICES=1 python run_embed.py --base_model /kaggle/input/qw14b-awq/transformers/default/1 --lora_path {lora_path} --input_text data.json --fold 1 2)\"\n",
    "import os\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d39800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:46:40.111043Z",
     "iopub.status.busy": "2024-11-26T06:46:40.110101Z",
     "iopub.status.idle": "2024-11-26T06:47:09.238653Z",
     "shell.execute_reply": "2024-11-26T06:47:09.237334Z"
    },
    "papermill": {
     "duration": 29.156178,
     "end_time": "2024-11-26T06:47:09.241638",
     "exception": false,
     "start_time": "2024-11-26T06:46:40.085460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 325/325 [03:36<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.pt.fold.0.2.embed\n",
      "data.pt.fold.1.2.embed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1694221771.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  text_to_embed.update(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import time\n",
    "text_to_embed = {}\n",
    "files = glob('*.pt*')\n",
    "while len(files) != 2:\n",
    "    time.sleep(1)\n",
    "    files = glob('*.pt*')\n",
    "\n",
    "\n",
    "time.sleep(3)    \n",
    "for path in files:\n",
    "    print(path)\n",
    "    text_to_embed.update(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82641137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.294697Z",
     "iopub.status.busy": "2024-11-26T06:47:09.294397Z",
     "iopub.status.idle": "2024-11-26T06:47:09.299556Z",
     "shell.execute_reply": "2024-11-26T06:47:09.298770Z"
    },
    "papermill": {
     "duration": 0.032492,
     "end_time": "2024-11-26T06:47:09.301173",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.268681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2596"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30d26804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.351687Z",
     "iopub.status.busy": "2024-11-26T06:47:09.351426Z",
     "iopub.status.idle": "2024-11-26T06:47:09.354817Z",
     "shell.execute_reply": "2024-11-26T06:47:09.354165Z"
    },
    "papermill": {
     "duration": 0.030277,
     "end_time": "2024-11-26T06:47:09.356358",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.326081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [text_to_embed[t] for t in new_queries][0.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22317209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.406615Z",
     "iopub.status.busy": "2024-11-26T06:47:09.406353Z",
     "iopub.status.idle": "2024-11-26T06:47:09.456901Z",
     "shell.execute_reply": "2024-11-26T06:47:09.456080Z"
    },
    "papermill": {
     "duration": 0.077719,
     "end_time": "2024-11-26T06:47:09.458586",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.380867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 5120]), torch.Size([2587, 5120]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings = torch.stack([text_to_embed[t] for t in new_queries])\n",
    "doc_embeddings = torch.stack([text_to_embed[t] for t in documents])\n",
    "query_embeddings.shape, doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e46302",
   "metadata": {
    "papermill": {
     "duration": 0.02453,
     "end_time": "2024-11-26T06:47:09.508213",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.483683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Eval (testing purpose only)\n",
    "This will not be ran in real submition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44f8fc60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.559287Z",
     "iopub.status.busy": "2024-11-26T06:47:09.558958Z",
     "iopub.status.idle": "2024-11-26T06:47:09.566099Z",
     "shell.execute_reply": "2024-11-26T06:47:09.565396Z"
    },
    "papermill": {
     "duration": 0.034535,
     "end_time": "2024-11-26T06:47:09.567689",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.533154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "def compute_metrics(q_embeds: torch.Tensor, d_embeds: torch.Tensor, target_ids: List[int]):\n",
    "    \"\"\"\n",
    "    Compute MAP@25 and Recall@100 metrics.\n",
    "    \n",
    "    Args:\n",
    "        q_embeds (torch.Tensor): Query embeddings of shape (M, dim), where M is the number of queries.\n",
    "        d_embeds (torch.Tensor): Document embeddings of shape (N, dim), where N is the number of documents.\n",
    "        target_ids (List[int]): List of target document indices (length M, one target index per query).\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints MAP@25 and Recall@100.\n",
    "    \"\"\"\n",
    "    # Compute similarity scores\n",
    "    scores = q_embeds @ d_embeds.T  # Shape: (M, N)\n",
    "\n",
    "    # Initialize variables for metrics\n",
    "    avg_precisions = []  # To store average precision for each query\n",
    "    recall_counts = []   # To store recall@100 counts for each query\n",
    "\n",
    "    # Compute metrics for each query\n",
    "    for i, target_id in enumerate(target_ids):\n",
    "        # Sort document indices by score in descending order\n",
    "        sorted_indices = torch.argsort(scores[i], descending=True)\n",
    "\n",
    "        # Compute precision@k and recall@100\n",
    "        relevant_docs = (sorted_indices[:100] == target_id).nonzero(as_tuple=True)[0]  # Find rank within top 100\n",
    "        recall_count = 1 if len(relevant_docs) > 0 else 0  # Check if target is in the top 100\n",
    "        recall_counts.append(recall_count)\n",
    "\n",
    "        # Compute average precision for top 25 (MAP@25)\n",
    "        precision_at_k = 0.0\n",
    "        num_relevant = 0\n",
    "        for rank, idx in enumerate(sorted_indices[:25]):\n",
    "            if idx == target_id:\n",
    "                num_relevant += 1\n",
    "                precision_at_k += num_relevant / (rank + 1)\n",
    "        avg_precisions.append(precision_at_k / 1 if num_relevant > 0 else 0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    map25 = sum(avg_precisions) / len(avg_precisions)\n",
    "    recall100 = sum(recall_counts) / len(recall_counts)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"MAP@25: {map25:.4f}\")\n",
    "    print(f\"Recall@100: {recall100:.4f}\")\n",
    "if not IS_SUBMISSION:\n",
    "    compute_metrics(query_embeddings, doc_embeddings, target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e124d6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.617917Z",
     "iopub.status.busy": "2024-11-26T06:47:09.617669Z",
     "iopub.status.idle": "2024-11-26T06:47:09.685067Z",
     "shell.execute_reply": "2024-11-26T06:47:09.684324Z"
    },
    "papermill": {
     "duration": 0.094603,
     "end_time": "2024-11-26T06:47:09.686901",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.592298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = query_embeddings @ doc_embeddings.T  # Shape: (M, N)\n",
    "sorted_indices = torch.argsort(scores,1, descending=True)[:,:25].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32fed996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.738675Z",
     "iopub.status.busy": "2024-11-26T06:47:09.738404Z",
     "iopub.status.idle": "2024-11-26T06:47:09.760557Z",
     "shell.execute_reply": "2024-11-26T06:47:09.759737Z"
    },
    "papermill": {
     "duration": 0.049973,
     "end_time": "2024-11-26T06:47:09.762352",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.712379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId_Answer</th>\n",
       "      <th>MisconceptionId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1869_B</td>\n",
       "      <td>706 1507 1345 2306 328 1672 1005 2518 1963 253...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1869_C</td>\n",
       "      <td>2306 1507 706 1005 1345 1999 2488 2532 987 251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1869_D</td>\n",
       "      <td>1005 328 1507 2532 1672 1516 706 1345 2306 248...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870_A</td>\n",
       "      <td>2142 2068 167 418 891 1755 979 113 1421 2256 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1870_B</td>\n",
       "      <td>2142 2068 167 891 341 979 1755 1871 143 418 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1870_C</td>\n",
       "      <td>2142 2068 167 891 1755 418 113 2078 143 979 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1871_A</td>\n",
       "      <td>1287 1073 2439 1665 2551 1306 1059 1098 1677 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1871_C</td>\n",
       "      <td>1287 1073 2439 1665 2551 1098 1059 912 1306 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1871_D</td>\n",
       "      <td>1073 1287 1059 1866 903 2471 912 2439 2064 167...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QuestionId_Answer                                    MisconceptionId\n",
       "0            1869_B  706 1507 1345 2306 328 1672 1005 2518 1963 253...\n",
       "1            1869_C  2306 1507 706 1005 1345 1999 2488 2532 987 251...\n",
       "2            1869_D  1005 328 1507 2532 1672 1516 706 1345 2306 248...\n",
       "3            1870_A  2142 2068 167 418 891 1755 979 113 1421 2256 3...\n",
       "4            1870_B  2142 2068 167 891 341 979 1755 1871 143 418 11...\n",
       "5            1870_C  2142 2068 167 891 1755 418 113 2078 143 979 26...\n",
       "6            1871_A  1287 1073 2439 1665 2551 1306 1059 1098 1677 1...\n",
       "7            1871_C  1287 1073 2439 1665 2551 1098 1059 912 1306 16...\n",
       "8            1871_D  1073 1287 1059 1866 903 2471 912 2439 2064 167..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input[\"MisconceptionId\"] = [\" \".join([str(x) for x in row]) for row in sorted_indices]\n",
    "df_input[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a1a34c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.821438Z",
     "iopub.status.busy": "2024-11-26T06:47:09.821063Z",
     "iopub.status.idle": "2024-11-26T06:47:09.825308Z",
     "shell.execute_reply": "2024-11-26T06:47:09.824476Z"
    },
    "papermill": {
     "duration": 0.033266,
     "end_time": "2024-11-26T06:47:09.827151",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.793885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# !pip uninstall -y torch\n",
    "\n",
    "# !pip install -q --no-index --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-vllm vllm\n",
    "\n",
    "# !pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "\n",
    "# !pip install -q -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "\n",
    "# !pip install -q --no-deps --no-index /kaggle/input/hf-libraries/sentence-transformers/sentence_transformers-3.1.0-py3-none-any.whl\n",
    "\n",
    "# !pip install --no-index /kaggle/input/pip-peft/peft-0.13.2-py3-none-any.whl\n",
    "\n",
    "# !pip install /kaggle/input/eedi-library/bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69b864bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T06:47:09.884398Z",
     "iopub.status.busy": "2024-11-26T06:47:09.884120Z",
     "iopub.status.idle": "2024-11-26T06:47:09.894374Z",
     "shell.execute_reply": "2024-11-26T06:47:09.893589Z"
    },
    "papermill": {
     "duration": 0.038264,
     "end_time": "2024-11-26T06:47:09.895926",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.857662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ### Apply QW32B\n",
    "# df_misconception_mapping.iloc[2532]['MisconceptionName']\n",
    "# #### BUILD INPUT PARQUET\n",
    "\n",
    "# df_input\n",
    "\n",
    "# from typing import List\n",
    "\n",
    "\n",
    "# misconceptions = df_misconception_mapping.MisconceptionName.values\n",
    "# def format_docs(MisconceptionId) -> str:\n",
    "#     doc_ids = [int(_) for _ in MisconceptionId.split(' ')]\n",
    "#     docs = [misconceptions[i] for i in doc_ids]\n",
    "#     docs = [f\"[{i}] \" + doc for i, doc in enumerate(docs)]\n",
    "#     return \"\\n\".join(docs)\n",
    "    \n",
    "# df_input['Retrival'] = df_input['MisconceptionId'].apply(format_docs)\n",
    "# print(df_input['Retrival'].iloc[0])\n",
    "\n",
    "\n",
    "# user_template = \"\"\"Instruction:\n",
    "# You will analyze a student's incorrect mathematical work and identify which misconception(s) from a provided list best explain their error.\n",
    "\n",
    "# **First, ensure you fully understand the problem by rewriting it in your own words to eliminate any potential ambiguities. Place your rewritten problem inside <clarified_problem> tags.**\n",
    "\n",
    "# **Next, verify the correctness of the student's answer by working through the problem step-by-step yourself, using the standard mathematical procedures and order of operations. Do not rely on the provided correct answer during this step. Place your solution inside <verification> tags.**\n",
    "\n",
    "# **As you perform your verification, pay close attention to the order in which operations should be performed according to BIDMAS/BODMAS/PEDMAS. Consider how changing the order affects the result, and note the impact of brackets on the calculation.**\n",
    "\n",
    "# After verifying, analyze the student's work carefully by considering:\n",
    "\n",
    "# 1. What specific error(s) did the student make?\n",
    "# 2. What pattern of thinking likely led to these errors?\n",
    "# 3. Which misconception(s) from the list best explain this thinking pattern?\n",
    "# 4. Are there multiple misconceptions that might apply?\n",
    "# 5. What evidence in the student's work supports each potential misconception?\n",
    "\n",
    "# **Be thorough in matching the student's error to the misconceptions provided. Consider whether the student is misunderstanding the priorities of operations, such as believing addition comes before multiplication.**\n",
    "\n",
    "# Write your analysis inside <thinking> tags. Keep your analysis under 500 words and focus on:\n",
    "\n",
    "# - The specific mathematical steps where errors occurred\n",
    "# - The likely reasoning behind these errors\n",
    "# - How specific misconceptions from the list explain this reasoning\n",
    "# - Why certain misconceptions fit better than others\n",
    "# - Whether multiple misconceptions might be at play\n",
    "\n",
    "# After your analysis, provide your conclusion inside <answer> tags by listing the index number(s) of the misconception(s) that best explain the student's error. If multiple misconceptions apply, list them in order of relevance, separated by commas.\n",
    "\n",
    "# **Ensure that your selected misconceptions directly correspond to the student's demonstrated errors and misconceptions, based on the evidence you've analyzed.**\n",
    "\n",
    "# Begin your analysis using the following inputs:\n",
    "\n",
    "\n",
    "\n",
    "# <misconception_list>\n",
    "# {Retrival}\n",
    "# </misconception_list>\n",
    "\n",
    "# <problem_statement>\n",
    "# {embed_prompt}\n",
    "# </problem_statement>\n",
    "# \"\"\"\n",
    "# assistant_prefix = '<clarified_problem> \\n'\n",
    "# def apply_template(row, tokenizer):\n",
    "#     user = user_template.format(embed_prompt=row[\"Prompt\"], Retrival=row[\"Retrival\"])\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": user,\n",
    "#         },\n",
    "#     ]\n",
    "#     text = tokenizer.apply_chat_template(\n",
    "#         messages, tokenize=False, add_generation_prompt=True\n",
    "#     )\n",
    "#     return text+assistant_prefix\n",
    "\n",
    "\n",
    "# df_input[\"text\"] = df_input.apply(lambda row: apply_template(row, tokenizer), 1)\n",
    "# df_input.to_parquet(\"input_llm.parquet\", index=False)\n",
    "# print(\"example input:\\n\\n\")\n",
    "# print(df_input.iloc[0]['text'])\n",
    "\n",
    "# example_msgs =  [{'role': 'user',\n",
    "#    'content': \"\\nInstruction:\\nYou will analyze a student's incorrect mathematical work and identify which misconception(s) from a provided list best explain their error.\\n\\n**First, ensure you fully understand the problem by rewriting it in your own words to eliminate any potential ambiguities. Place your rewritten problem inside <clarified_problem> tags.**\\n\\n**Next, verify the correctness of the student's answer by working through the problem step-by-step yourself, using the standard mathematical procedures and order of operations. Do not rely on the provided correct answer during this step. Place your solution inside <verification> tags.**\\n\\n**As you perform your verification, pay close attention to the order in which operations should be performed according to BIDMAS/BODMAS/PEDMAS. Consider how changing the order affects the result, and note the impact of brackets on the calculation.**\\n\\nAfter verifying, analyze the student's work carefully by considering:\\n\\n1. What specific error(s) did the student make?\\n2. What pattern of thinking likely led to these errors?\\n3. Which misconception(s) from the list best explain this thinking pattern?\\n4. Are there multiple misconceptions that might apply?\\n5. What evidence in the student's work supports each potential misconception?\\n\\n**Be thorough in matching the student's error to the misconceptions provided. Consider whether the student is misunderstanding the priorities of operations, such as believing addition comes before multiplication.**\\n\\nWrite your analysis inside <thinking> tags. Keep your analysis under 500 words and focus on:\\n\\n- The specific mathematical steps where errors occurred\\n- The likely reasoning behind these errors\\n- How specific misconceptions from the list explain this reasoning\\n- Why certain misconceptions fit better than others\\n- Whether multiple misconceptions might be at play\\n\\nAfter your analysis, provide your conclusion inside <answer> tags by listing the index number(s) of the misconception(s) that best explain the student's error. If multiple misconceptions apply, list them in order of relevance, separated by commas.\\n\\n**Ensure that your selected misconceptions directly correspond to the student's demonstrated errors and misconceptions, based on the evidence you've analyzed.**\\n\\nBegin your analysis using the following inputs:\\n\\n\\n\\n<misconception_list>\\n[0] Answers order of operations questions with brackets as if the brackets are not there\\n[1] Confuses the order of operations, believes subtraction comes before multiplication \\n[2] Believes order of operations does not affect the answer to a calculation\\n[3] Applies BIDMAS in strict order (does not realize addition and subtraction, and multiplication and division, are of equal priority)\\n[4] Confuses the order of operations, believes addition comes before multiplication \\n[5] Has removed brackets but not performed the operation\\n[6] Carries out operations from left to right regardless of priority order\\n[7] Has not realised that the answer may be changed by the insertion of brackets\\n[8] Carries out operations from right to left regardless of priority order\\n[9] May have made a calculation error using the order of operations\\n[10] Misunderstands order of operations in algebraic expressions\\n[11] Inserts brackets but not changed order of operation\\n[12] Carries out operations from left to right regardless of priority order, unless brackets are used\\n[13] Has completed only one of the two operations.\\n[14] Does not realize we can omit the multiplication symbol in algebraic expressions\\n[15] Performs addition ahead of multiplication\\n[16] Does not include brackets when attempting to multiply an expression\\n[17] Does not correctly apply the distributive property of multiplication\\n[18] Thinks multiplication and addition are the same\\n[19] Does not include brackets when required\\n[20] Performs subtraction right to left if priority order means doing a calculation to the right first\\n[21] Performs addition ahead of any other operation\\n[22] Does not interpret the correct order of operations from a worded problem\\n[23] Multiplies before applying a power \\n[24] Does not correctly apply the commutative property of multiplication\\n</misconception_list>\\n\\n<problem_statement>\\nQuestion: \\\\[\\n3 \\\\times 2+4-5\\n\\\\]\\nWhere do the brackets need to go to make the answer equal \\\\( 13 \\\\) ?\\n\\nSubjectName: BIDMAS\\nConstructName: Use the order of operations to carry out calculations involving powers\\nCorrect text: \\\\( 3 \\\\times(2+4)-5 \\\\)\\nStudent wrong answer : Does not need brackets\\n</problem_statement>\\n\"},\n",
    "#   {'role': 'assistant',\n",
    "\n",
    "#    'content': '<clarified_problem> \\nQuestion: Solve the expression \\\\( 3 \\\\times 2 + 4 - 5 \\\\) after placing brackets to make the result equal to 13.\\n</clarified_problem>\\n\\n<verification>\\nTo verify the solution, I will follow the standard order of operations (BIDMAS/BODMAS/PEDMAS):\\n\\n- Without any brackets, the expression is evaluated as:\\n  \\\\[\\n  3 \\\\times 2 + 4 - 5 = 6 + 4 - 5 = 10 - 5 = 5\\n  \\\\]\\n  This does not yield 13.\\n\\n- With the brackets placed as \\\\( 3 \\\\times (2+4) - 5 \\\\):\\n  \\\\[\\n  3 \\\\times (2+4) - 5 = 3 \\\\times 6 - 5 = 18 - 5 = 13\\n  \\\\]\\n  This correctly yields 13.\\n\\nTherefore, the correct placement of brackets is \\\\( 3 \\\\times (2+4) - 5 \\\\).\\n</verification>\\n\\n<thinking>\\nThe student provided the answer \"Does not need brackets,\" which incorrectly suggests they believe the expression as written is already equal to 13 without the need for reordering operations with brackets.\\n\\n1. **Specific Error**: The student\\'s error lies in their belief that the expression \\\\( 3 \\\\times 2 + 4 - 5 \\\\) does not need brackets to achieve the result of 13. This implies they are not recognizing the necessity of changing the order of operations using brackets.\\n\\n2. **Pattern of Thinking**: The student likely believes that addition comes before multiplication, leading them to incorrectly conclude that no brackets are necessary. They may also be misunderstanding the importance of the order of operations, thinking it can be ignored in this context.\\n\\n3. **Misconceptions from the List**:\\n   - **Misconception 4**: Confuses the order of operations, believing addition comes before multiplication. This fits as the student is performing the addition before multiplication.\\n   - **Misconception 6**: Carries out operations from left to right regardless of priority order. This also aligns with the student’s approach of evaluating the expression without proper order of operations.\\n\\n4. **Evidence in Student\\'s Work**: The student\\'s conclusion that no brackets are needed suggests a misunderstanding of the order of operations and the necessity of using brackets to achieve the desired result. Their belief that addition should come before multiplication is evident from the incorrect evaluation of the expression without brackets.\\n\\n5. **Relevance of Misconceptions**: Misconception 4 directly addresses the student\\'s belief that addition should come before multiplication. Misconception 6 further reinforces the idea that the student is not recognizing the priority of operations and is instead evaluating the expression from left to right.\\n\\nGiven this analysis, Misconception 4 and Misconception 6 best explain the student\\'s error.\\n</thinking>\\n\\n<answer>\\n4, 6\\n</answer>'}]\n",
    "\n",
    "# %%writefile run_vllm.py\n",
    "\n",
    "# import re\n",
    "\n",
    "# import vllm\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.read_parquet(\"input_llm.parquet\")\n",
    "\n",
    "\n",
    "\n",
    "# model_path = '/kaggle/input/qw32-awq-2011/transformers/default/1'\n",
    "\n",
    "# llm = vllm.LLM(\n",
    "#     model_path,\n",
    "#     quantization=\"awq\",\n",
    "#     tensor_parallel_size=2,\n",
    "#     gpu_memory_utilization=0.95, \n",
    "#     trust_remote_code=True,\n",
    "#     dtype=\"half\", \n",
    "#     enforce_eager=True,\n",
    "#     max_model_len=5000,\n",
    "#     disable_log_stats=False,\n",
    "#     enable_prefix_caching=True\n",
    "# )\n",
    "# tokenizer = llm.get_tokenizer()\n",
    "\n",
    "\n",
    "# responses = llm.generate(\n",
    "#     df[\"text\"].values,\n",
    "#     vllm.SamplingParams(\n",
    "#         n=1,  # Number of output sequences to return for each prompt.\n",
    "#         top_p=0.8,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "#         temperature=0.0,  # randomness of the sampling\n",
    "#         seed=777, # Seed for reprodicibility\n",
    "#         skip_special_tokens=False,  # Whether to skip special tokens in the output.\n",
    "#         max_tokens=1024,  # Maximum number of tokens to generate per output sequence.\n",
    "#     ),\n",
    "#     use_tqdm=True\n",
    "# )\n",
    "\n",
    "# responses = [x.outputs[0].text for x in responses]\n",
    "# df[\"fullLLMText\"] = responses\n",
    "\n",
    "# # def extract_response(text):\n",
    "# #     return \",\".join(re.findall(r\"<response>(.*?)</response>\", text)).strip()\n",
    "\n",
    "# # df[\"llmMisconception\"] = responses\n",
    "# df.to_parquet(\"output_llm.parquet\", index=False)\n",
    "\n",
    "# ### RUN LLM\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# import gc; gc.collect()\n",
    "\n",
    "# !python run_vllm.py\n",
    "\n",
    "# ### POST PROCESS\n",
    "# LLM output -> submission.csv\n",
    "\n",
    "# import re\n",
    "\n",
    "# def extract_tag(s, tag=\"answer\"):\n",
    "#     \"\"\"\n",
    "#     Extracts text enclosed within specified tags (e.g., <answer>...</answer>).\n",
    "\n",
    "#     Args:\n",
    "#         s (str): The input string containing the tags.\n",
    "#         tag (str): The tag name to extract the text from (default is \"answer\").\n",
    "\n",
    "#     Returns:\n",
    "#         str: The text enclosed by the specified tags, or None if no match is found.\n",
    "#     \"\"\"\n",
    "#     pattern = rf\"<{tag}>(.*?)</{tag}>\"  # Regex to match the tag and its content\n",
    "#     match = re.search(pattern, s, re.DOTALL)  # Allow multiline matching\n",
    "#     if match:\n",
    "#         return match.group(1)  # Return the text inside the tags\n",
    "#     return None\n",
    "\n",
    "# # Example usage\n",
    "# text = \"Here is some text with <answer>This is the target text</answer> in it.\"\n",
    "# result = extract_tag(text, tag=\"answer\")\n",
    "# print(result)  # Output: This is the target text\n",
    "\n",
    "\n",
    "\n",
    "# llm_output = pd.read_parquet(\"output_llm.parquet\")\n",
    "\n",
    "# all_new_ids = []\n",
    "\n",
    "# for idx, row in llm_output.iterrows():\n",
    "#     try:\n",
    "#         response = row['fullLLMText']\n",
    "#         answer = extract_tag(response)\n",
    "#         ids = [int(i.strip()) for i in answer.strip().split(',') if i.strip().isdigit()]\n",
    "#         print(f\"Question ID: {row['QuestionId_Answer']}, Misconception IDs: {ids}\")\n",
    "    \n",
    "#         first_retrived_ids = [int(i) for i in row['MisconceptionId'].split()]\n",
    "#         print(f'Before: {first_retrived_ids}')\n",
    "\n",
    "\n",
    "#         # Step 2: Insert LLM predicted item to the list\n",
    "#         priority_misconception_ids = []\n",
    "#         for priority_id in ids:\n",
    "#             priority_misconception_ids.append(first_retrived_ids.pop(priority_id))\n",
    "    \n",
    "#         new_ids = priority_misconception_ids+first_retrived_ids\n",
    "#         print('After:', new_ids)\n",
    "#         all_new_ids.append(' '.join([str(i) for i in new_ids]))\n",
    "#         print(\"===\"*6)\n",
    "#     except Exception as e:\n",
    "#         all_new_ids.append(row['MisconceptionId'])\n",
    "#         print(f'{idx=},\\n {row=}\\nError:{e}')\n",
    "\n",
    "# df_input_llm = df_input.copy()\n",
    "# df_input_llm['MisconceptionId'] = all_new_ids\n",
    "# df_input_llm[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "# pd.read_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b35c1",
   "metadata": {
    "papermill": {
     "duration": 0.024334,
     "end_time": "2024-11-26T06:47:09.944965",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.920631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07e3ea",
   "metadata": {
    "papermill": {
     "duration": 0.024298,
     "end_time": "2024-11-26T06:47:09.993711",
     "exception": false,
     "start_time": "2024-11-26T06:47:09.969413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f16a3",
   "metadata": {
    "papermill": {
     "duration": 0.024939,
     "end_time": "2024-11-26T06:47:10.043044",
     "exception": false,
     "start_time": "2024-11-26T06:47:10.018105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c614db",
   "metadata": {
    "papermill": {
     "duration": 0.025352,
     "end_time": "2024-11-26T06:47:10.093816",
     "exception": false,
     "start_time": "2024-11-26T06:47:10.068464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4581967,
     "sourceId": 10009171,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 209651782,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 169032,
     "modelInstanceId": 146504,
     "sourceId": 172110,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171421,
     "modelInstanceId": 148911,
     "sourceId": 174909,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171434,
     "modelInstanceId": 148923,
     "sourceId": 174921,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171497,
     "modelInstanceId": 148996,
     "sourceId": 175000,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 174732,
     "modelInstanceId": 152276,
     "sourceId": 178762,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 409.898595,
   "end_time": "2024-11-26T06:47:12.673711",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-26T06:40:22.775116",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
